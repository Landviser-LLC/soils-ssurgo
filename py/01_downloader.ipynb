{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate download of the zipped county files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the webpage (per state) automate the download of the ZIP county files as outlined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Manually - Download .html for MO, MS, TX, LA (AR is done) - DONE\\n2. From Option 0 routine make a function to go over each state .html, extract list of .zip and export as CSV\\n3. Append all CSV together\\n4. Make a log file with STATE | FIPS | Version date | Date processed\\n5. Function to download files for each link (checks log if zip file already exists/processed or link is newer)\\n\\nNote: I changed ABSOLUTE links to \"Trubin/...\" to the relative links to the folders in this REPO - use it as example\\n      for future work.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 0\n",
    "###\n",
    "# parsing HTML code (downloaded from  USDA webapp pereodically - once a month? to check for updates there and download-unzip-ingest new versions of the files:\n",
    "# this is the main URL where web app resides - it generates the table with county lists and timestamps when updated\n",
    "# https://websoilsurvey.sc.egov.usda.gov/App/WebSoilSurvey.aspx \n",
    "# the source file download to /data_in/html/\n",
    "\n",
    "# Lesha - TO DO:\n",
    "'''\n",
    "1. Manually - Download .html for MO, MS, TX, LA (AR is done) - DONE\n",
    "2. From Option 0 routine make a function to go over each state .html, extract list of .zip and export as CSV\n",
    "3. Append all CSV together\n",
    "4. Make a log file with STATE | FIPS | Version date | Date processed\n",
    "5. Function to download files for each link (checks log if zip file already exists/processed or link is newer)\n",
    "\n",
    "Note: I changed ABSOLUTE links to \"Trubin/...\" to the relative links to the folders in this REPO - use it as example\n",
    "      for future work.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lxml - that was needed for pd.read_html() - solved without it so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the whole html of 12K+ rows apparently only has 20 AK (Alaska) counties - so apparently the HTML need to be manually\n",
    "# pulled per state at least - that seems to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220715_view-source_https___websoilsurvey.sc.egov.usda.gov_App_WebSoilSurvey.aspx.html\n",
      "20220722-AK-view-source_https___websoilsurvey.sc.egov.usda.gov_App_WebSoilSurvey.aspx.html\n",
      "20220722-AR.html\n",
      "20220726-LA.html\n",
      "20220726-MO.html\n",
      "20220726-MS.html\n",
      "20220726-TX.html\n"
     ]
    }
   ],
   "source": [
    "#all HTML files in directory\n",
    "os.chdir(\"../html\")\n",
    "for html_files in glob.glob(\"*.html\"):\n",
    "    print(html_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20220715_view-source_https___websoilsurvey.sc.egov.usda.gov_App_WebSoilSurvey.aspx.html', '20220722-AK-view-source_https___websoilsurvey.sc.egov.usda.gov_App_WebSoilSurvey.aspx.html', '20220722-AR.html', '20220726-LA.html', '20220726-MO.html', '20220726-MS.html', '20220726-TX.html']\n"
     ]
    }
   ],
   "source": [
    "html_files = [\"*.html\"]\n",
    "files = []\n",
    "for type in html_files:\n",
    "     this_type_files = glob.glob(type)\n",
    "     files += this_type_files\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_rec = glob.glob('D:\\python\\*.html')\n",
    "##print(all_rec)\n",
    "#list_data = []\n",
    "#\n",
    "#for filename in all_rec:\n",
    "#    data = pd.read_html(filename)\n",
    "#    list_data.append(data)\n",
    "#\n",
    "#list_data # showing array of all html data  \n",
    "#\n",
    "##remove this line\n",
    "#list_data =list_data[0] \n",
    "##remove this line\n",
    "#list_data =list_data[-1] \n",
    "#\n",
    "#pd.DataFrame(list_data).reset_index(drop=True) #replace concat with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_1080/1126265189.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\trubin\\AppData\\Local\\Temp/ipykernel_1080/1126265189.py\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    print os.path.join(directory, file)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "root_dir = '../html'\n",
    "\n",
    "for directory, subdirectories, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        print os.path.join(directory, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Make function out of below and apply to folder with HTML files\n",
    "'''\n",
    "\n",
    "#Make a list from big HTML file\n",
    "#list = [ line for line in open(r'../data_in/html/20220715_view-source_https___websoilsurvey.sc.egov.usda.gov_App_WebSoilSurvey.aspx.html') if '.zip' in line]\n",
    "# testing per state HTML - AK\n",
    "# list = [ line for line in open(r'../data_in/html/20220722-AK-view-source_https___websoilsurvey.sc.egov.usda.gov_App_WebSoilSurvey.aspx.html') if '.zip' in line]\n",
    "\n",
    "# testing per state HTML - AR\n",
    "list = [ line for line in open(r'../data_in/html/20220722-AR.html') if '.zip' in line]\n",
    "#List to dataframe\n",
    "df = pd.DataFrame(list)\n",
    "#name the column\n",
    "df.columns = ['1']\n",
    "#data cleaning\n",
    "df['1'].str.split('                                                          ', expand=True)\n",
    "df = df[df['1'].str.contains('TemplateDB') == False]\n",
    "df['1'] = df['1'].map(lambda x: x.lstrip('</td></tr><tr><td class=\"line-number\" value=\"'))\n",
    "df[['1', '2']] = df['1'].str.split('                                                      <span class=\"html-tag\">&lt;a <span class=\"html-attribute-name\">href</span>=\"<a class=\"html-attribute-value html-external-link\" target=\"_blank\" href=\"', expand=True)\n",
    "df = df.drop('1', axis=1)\n",
    "df = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "df['1'] = df['2'].str.split('\" rel=').str[0]\n",
    "df['2'] = df['1'].str.split('\" rel=').str[0]\n",
    "df = df.drop('1', axis=1)\n",
    "#result export to CSV\n",
    "df.to_csv('../data_out/AR_links_list.csv', index=False)  \n",
    "\n",
    "# when making function, maybe df append of all records? - see example in cell below from SmartRice project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to clean and append files per consultant\n",
    "\n",
    "def to_df_csv_path(path):\n",
    "    '''\n",
    "    accepts path to folder with CSVs all containing the same structured tables for a single consultant (Consultant ID as a string)\n",
    "    1. define a list of CSV files from the path\n",
    "    for each file in the list:\n",
    "    1. load csv as df\n",
    "    2. drop empty columns\n",
    "    3. for string columns - strip beginning/end spaces\n",
    "    4. append all together.-> return df\n",
    "    '''\n",
    "    csvlst = os.listdir(path)\n",
    "    print(csvlst)\n",
    "    os.chdir(path)\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    for file in range(len(csvlst)):\n",
    "        if df1.empty:\n",
    "            df1 = pd.read_csv(csvlst[file])\n",
    "            file+=1\n",
    "        else:\n",
    "            df2 = pd.read_csv(csvlst[file])\n",
    "            df1 = df1.append(df2, ignore_index=True, sort = True) # sort = True is added after received warning about further versions of Anaconda/Pandas\n",
    "            file+=1\n",
    "    \n",
    "      \n",
    "    # cleaning '---' records (this is specific for this data set, can be generalized later\n",
    "    df1.replace({'---': None}, inplace=True)\n",
    "    \n",
    "    # dropping empty columns\n",
    "    df1.dropna(axis=1, how='all', inplace=True)\n",
    "    \n",
    "    # counting and then dropping completely duplicated records - sometimes consultants report previous year with the records of current year, \n",
    "    #thus providing complete duplicate for some records\n",
    "    print('There were completely duplicated rows before cleanup: ', df1.duplicated().sum())\n",
    "    #dfdup = df1.loc[df1.duplicated(keep=False), :]\n",
    "    df1.drop_duplicates(keep='first', inplace=True)\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 70 entries, 0 to 138\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   2       70 non-null     object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "# 137 counties for AK\n",
    "# 70 counties for AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "#os.makedirs('../soils-ssurgo', exist_ok=True)\n",
    "df.to_csv('../data_out/AR_links_list.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1\n",
    "# Read zip files from page, download file, extract and stream output\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "import os,sys,requests,csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# check for download directory existence; create if not there\n",
    "if not os.path.isdir('C:\\\\Users\\\\trubin\\\\Downloads'):\n",
    "    os.makedirs('C:\\\\Users\\\\trubin\\\\Downloads')\n",
    "\n",
    "# Get labels and zip file download links\n",
    "mainurl = \"https://websoilsurvey.sc.egov.usda.gov\" # ??\n",
    "#\n",
    "\n",
    "# below is example of cached URL for zipfile with MS Access template included - we do not need it\n",
    "# https://websoilsurvey.sc.egov.usda.gov/DSD/Download/Cache/SSA/wss_SSA_AL001_soildb_US_2003_[2021-09-15].zip\n",
    "\n",
    "url = \"http://websoilsurvey.sc.egov.usda.gov/DSD/Download/{CACHENAME}/{SUBFOLDER}/{FILENAME}\"\n",
    "\n",
    "# get page and setup BeautifulSoup\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "\n",
    "# Get all file labels and filter so only use CSVs\n",
    "mainlabel = soup.find_all(\"td\", {\"class\": \"labelOptional_ind\"})\n",
    "for td in mainlabel:\n",
    "    if \"_AL\" in td.text:\n",
    "        print(td.text)\n",
    "\n",
    "###\n",
    "\n",
    "# Get all <a href> urls\n",
    "for link in soup.find_all('a'):\n",
    "    print(mainurl + link.get('href'))\n",
    "\n",
    "# QUESTION: HOW CAN I LOOP THROUGH ALL FILE LABELS AND FIND ONLY THE\n",
    "# CSV LABELS AND THEIR CORRESPONDING ZIP DOWNLOAD LINK, SKIPPING ANY\n",
    "# XML LABELS/LINKS, THEN LOOP AND EXECUTE THE CODE BELOW FOR EACH, \n",
    "# REPLACING zipfilename WITH THE MAIN LABEL AND zipurl WITH THE ZIP \n",
    "# DOWNLOAD LINK?\n",
    "\n",
    "# Test downloading and streaming\n",
    "zipfilename = \"wss_SSA_AL019_[2021-09-15].zip\"\n",
    "zipurl = \"http://websoilsurvey.sc.egov.usda.gov/DSD/Download/{CACHENAME}/{SUBFOLDER}/{FILENAME}\"\n",
    "outputFilename = \"C:\\\\Users\\\\trubin\\\\Downloads\" + zipfilename\n",
    "\n",
    "# Unzip and stream CSV file\n",
    "url = urllib.request.urlopen(zipurl)\n",
    "zippedData = url.read()\n",
    "\n",
    "# Save zip file to disk\n",
    "print (\"Saving to \",outputFilename)\n",
    "output = open(outputFilename,'wb')\n",
    "output.write(zippedData)\n",
    "output.close()\n",
    "\n",
    "# Unzip and stream CSV file\n",
    "with ZipFile(BytesIO(zippedData)) as my_zip_file:\n",
    "   for contained_file in my_zip_file.namelist():\n",
    "    with open((\"unzipped_and_read_\" + contained_file + \".file\"), \"wb\") as output:\n",
    "        for line in my_zip_file.open(contained_file).readlines():\n",
    "            print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 2\n",
    "import urllib.request\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "\n",
    "url = \"http://websoilsurvey.sc.egov.usda.gov\"\n",
    "headers={'User-Agent':user_agent,} \n",
    "\n",
    "request=urllib.request.Request(url,None,headers) #The assembled request\n",
    "response = urllib.request.urlopen(request)\n",
    "data = response.read() # The data u need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 3\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-Agent', 'MyApp/1.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "urllib.request.urlretrieve(\n",
    "  \"http://websoilsurvey.sc.egov.usda.gov/DSD/Download/{CACHENAME}/{SUBFOLDER}\",\n",
    "   \"wss_SSA_AL019_[2021-09-15].zip\")\n",
    "outputFilename = \"C:\\\\Users\\\\trubin\\\\Downloads\" + zipfilename\n",
    "url = urllib.request.urlopen(zipurl)\n",
    "zippedData = url.read()\n",
    "# Save zip file to disk\n",
    "print (\"Saving to \",outputFilename)\n",
    "output = open(outputFilename,'wb')\n",
    "output.write(zippedData)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create txt log of zipfiles downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not download the file if already in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not in the log or the file on website is newer\n",
    "\n",
    "# 1. download\n",
    "# 2. update log\n",
    "# 3. unzip to SSURGO folder\n",
    "# 4. delete the downloaded zipped file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdbe6a345e49dbba1d6ce3231a5791291bc277e174161bd9129a45264752b2ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
