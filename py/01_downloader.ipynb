{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate download of the zipped county files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement urllib (from versions: none)\n",
      "ERROR: No matching distribution found for urllib\n"
     ]
    }
   ],
   "source": [
    "pip install urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the webpage (per state) automate the download of the ZIP county files as outlined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 0\n",
    "###\n",
    "# parsing HTML code (downloaded from  USDA webapp pereodically - once a month? to check for updates there and download-unzip-ingest new versions of the files:\n",
    "# this is the main URL where web app resides - it generates the table with county lists and timestamps when updated\n",
    "# https://websoilsurvey.sc.egov.usda.gov/App/WebSoilSurvey.aspx \n",
    "# the source file download to /data_in/html/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1\n",
    "# Read zip files from page, download file, extract and stream output\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "import os,sys,requests,csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# check for download directory existence; create if not there\n",
    "if not os.path.isdir('C:\\\\Users\\\\trubin\\\\Downloads'):\n",
    "    os.makedirs('C:\\\\Users\\\\trubin\\\\Downloads')\n",
    "\n",
    "# Get labels and zip file download links\n",
    "mainurl = \"https://websoilsurvey.sc.egov.usda.gov\" # ??\n",
    "#\n",
    "\n",
    "# below is example of cached URL for zipfile with MS Access template included - we do not need it\n",
    "# https://websoilsurvey.sc.egov.usda.gov/DSD/Download/Cache/SSA/wss_SSA_AL001_soildb_US_2003_[2021-09-15].zip\n",
    "\n",
    "url = \"http://websoilsurvey.sc.egov.usda.gov/DSD/Download/{CACHENAME}/{SUBFOLDER}/{FILENAME}\"\n",
    "\n",
    "# get page and setup BeautifulSoup\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "\n",
    "# Get all file labels and filter so only use CSVs\n",
    "mainlabel = soup.find_all(\"td\", {\"class\": \"labelOptional_ind\"})\n",
    "for td in mainlabel:\n",
    "    if \"_AL\" in td.text:\n",
    "        print(td.text)\n",
    "\n",
    "###\n",
    "\n",
    "# Get all <a href> urls\n",
    "for link in soup.find_all('a'):\n",
    "    print(mainurl + link.get('href'))\n",
    "\n",
    "# QUESTION: HOW CAN I LOOP THROUGH ALL FILE LABELS AND FIND ONLY THE\n",
    "# CSV LABELS AND THEIR CORRESPONDING ZIP DOWNLOAD LINK, SKIPPING ANY\n",
    "# XML LABELS/LINKS, THEN LOOP AND EXECUTE THE CODE BELOW FOR EACH, \n",
    "# REPLACING zipfilename WITH THE MAIN LABEL AND zipurl WITH THE ZIP \n",
    "# DOWNLOAD LINK?\n",
    "\n",
    "# Test downloading and streaming\n",
    "zipfilename = \"wss_SSA_AL019_[2021-09-15].zip\"\n",
    "zipurl = \"http://websoilsurvey.sc.egov.usda.gov/DSD/Download/{CACHENAME}/{SUBFOLDER}/{FILENAME}\"\n",
    "outputFilename = \"C:\\\\Users\\\\trubin\\\\Downloads\" + zipfilename\n",
    "\n",
    "# Unzip and stream CSV file\n",
    "url = urllib.request.urlopen(zipurl)\n",
    "zippedData = url.read()\n",
    "\n",
    "# Save zip file to disk\n",
    "print (\"Saving to \",outputFilename)\n",
    "output = open(outputFilename,'wb')\n",
    "output.write(zippedData)\n",
    "output.close()\n",
    "\n",
    "# Unzip and stream CSV file\n",
    "with ZipFile(BytesIO(zippedData)) as my_zip_file:\n",
    "   for contained_file in my_zip_file.namelist():\n",
    "    with open((\"unzipped_and_read_\" + contained_file + \".file\"), \"wb\") as output:\n",
    "        for line in my_zip_file.open(contained_file).readlines():\n",
    "            print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 2\n",
    "import urllib.request\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "\n",
    "url = \"http://websoilsurvey.sc.egov.usda.gov\"\n",
    "headers={'User-Agent':user_agent,} \n",
    "\n",
    "request=urllib.request.Request(url,None,headers) #The assembled request\n",
    "response = urllib.request.urlopen(request)\n",
    "data = response.read() # The data u need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 3\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-Agent', 'MyApp/1.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "urllib.request.urlretrieve(\n",
    "  \"http://websoilsurvey.sc.egov.usda.gov/DSD/Download/{CACHENAME}/{SUBFOLDER}\",\n",
    "   \"wss_SSA_AL019_[2021-09-15].zip\")\n",
    "outputFilename = \"C:\\\\Users\\\\trubin\\\\Downloads\" + zipfilename\n",
    "url = urllib.request.urlopen(zipurl)\n",
    "zippedData = url.read()\n",
    "# Save zip file to disk\n",
    "print (\"Saving to \",outputFilename)\n",
    "output = open(outputFilename,'wb')\n",
    "output.write(zippedData)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create txt log of zipfiles downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not download the file if already in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not in the log or the file on website is newer\n",
    "\n",
    "# 1. download\n",
    "# 2. update log\n",
    "# 3. unzip to SSURGO folder\n",
    "# 4. delete the downloaded zipped file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('.venv39': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c310f0b89e2e21f775d6a0da44d32d2ad8a4b5ca6fce96848d4d6666866aca07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
