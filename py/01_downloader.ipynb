{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate download of the zipped county files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the webpage (per state) automate the download of the ZIP county files as outlined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 0\n",
    "###\n",
    "# parsing HTML code (downloaded from  USDA webapp pereodically - once a month? to check for updates there and download-unzip-ingest new versions of the files:\n",
    "# this is the main URL where web app resides - it generates the table with county lists and timestamps when updated\n",
    "# https://websoilsurvey.sc.egov.usda.gov/App/WebSoilSurvey.aspx \n",
    "# the source file download to /data_in/html/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    2\n",
       "0   https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "2   https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "4   https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "6   https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "8   https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "10  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "12  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "14  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "16  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "18  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "20  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "22  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "24  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "26  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "28  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "30  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "32  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "34  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "36  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow...\n",
       "38  https://websoilsurvey.sc.egov.usda.gov/DSD/Dow..."
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "\n",
    "#Make a list from big HTML file\n",
    "list = [ line for line in open(r'C:\\Users\\trubin\\Desktop\\Landviser LLC\\git repos\\soils-ssurgo\\data_in\\html\\20220715_view-source_https___websoilsurvey.sc.egov.usda.gov_App_WebSoilSurvey.aspx.html') if '.zip' in line]\n",
    "#List to dataframe\n",
    "df = pd.DataFrame(list)\n",
    "#name the column\n",
    "df.columns = ['1']\n",
    "#data cleaning\n",
    "df['1'].str.split('                                                          ', expand=True)\n",
    "df = df[df['1'].str.contains('TemplateDB') == False]\n",
    "df['1'] = df['1'].map(lambda x: x.lstrip('</td></tr><tr><td class=\"line-number\" value=\"'))\n",
    "df[['1', '2']] = df['1'].str.split('                                                      <span class=\"html-tag\">&lt;a <span class=\"html-attribute-name\">href</span>=\"<a class=\"html-attribute-value html-external-link\" target=\"_blank\" href=\"', expand=True)\n",
    "df = df.drop('1', axis=1)\n",
    "df = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "df['1'] = df['2'].str.split('\" rel=').str[0]\n",
    "df['2'] = df['1'].str.split('\" rel=').str[0]\n",
    "df = df.drop('1', axis=1)\n",
    "#result\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to csv\n",
    "os.makedirs('C:/Users/trubin/Desktop/Landviser LLC/git repos/soils-ssurgo', exist_ok=True)\n",
    "df.to_csv('C:/Users/trubin/Desktop/Landviser LLC/git repos/soils-ssurgo/data_out/links_list2.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 1\n",
    "# Read zip files from page, download file, extract and stream output\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import urllib.request\n",
    "import os,sys,requests,csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# check for download directory existence; create if not there\n",
    "if not os.path.isdir('C:\\\\Users\\\\trubin\\\\Downloads'):\n",
    "    os.makedirs('C:\\\\Users\\\\trubin\\\\Downloads')\n",
    "\n",
    "# Get labels and zip file download links\n",
    "mainurl = \"https://websoilsurvey.sc.egov.usda.gov\" # ??\n",
    "#\n",
    "\n",
    "# below is example of cached URL for zipfile with MS Access template included - we do not need it\n",
    "# https://websoilsurvey.sc.egov.usda.gov/DSD/Download/Cache/SSA/wss_SSA_AL001_soildb_US_2003_[2021-09-15].zip\n",
    "\n",
    "url = \"http://websoilsurvey.sc.egov.usda.gov/DSD/Download/{CACHENAME}/{SUBFOLDER}/{FILENAME}\"\n",
    "\n",
    "# get page and setup BeautifulSoup\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "\n",
    "# Get all file labels and filter so only use CSVs\n",
    "mainlabel = soup.find_all(\"td\", {\"class\": \"labelOptional_ind\"})\n",
    "for td in mainlabel:\n",
    "    if \"_AL\" in td.text:\n",
    "        print(td.text)\n",
    "\n",
    "###\n",
    "\n",
    "# Get all <a href> urls\n",
    "for link in soup.find_all('a'):\n",
    "    print(mainurl + link.get('href'))\n",
    "\n",
    "# QUESTION: HOW CAN I LOOP THROUGH ALL FILE LABELS AND FIND ONLY THE\n",
    "# CSV LABELS AND THEIR CORRESPONDING ZIP DOWNLOAD LINK, SKIPPING ANY\n",
    "# XML LABELS/LINKS, THEN LOOP AND EXECUTE THE CODE BELOW FOR EACH, \n",
    "# REPLACING zipfilename WITH THE MAIN LABEL AND zipurl WITH THE ZIP \n",
    "# DOWNLOAD LINK?\n",
    "\n",
    "# Test downloading and streaming\n",
    "zipfilename = \"wss_SSA_AL019_[2021-09-15].zip\"\n",
    "zipurl = \"http://websoilsurvey.sc.egov.usda.gov/DSD/Download/{CACHENAME}/{SUBFOLDER}/{FILENAME}\"\n",
    "outputFilename = \"C:\\\\Users\\\\trubin\\\\Downloads\" + zipfilename\n",
    "\n",
    "# Unzip and stream CSV file\n",
    "url = urllib.request.urlopen(zipurl)\n",
    "zippedData = url.read()\n",
    "\n",
    "# Save zip file to disk\n",
    "print (\"Saving to \",outputFilename)\n",
    "output = open(outputFilename,'wb')\n",
    "output.write(zippedData)\n",
    "output.close()\n",
    "\n",
    "# Unzip and stream CSV file\n",
    "with ZipFile(BytesIO(zippedData)) as my_zip_file:\n",
    "   for contained_file in my_zip_file.namelist():\n",
    "    with open((\"unzipped_and_read_\" + contained_file + \".file\"), \"wb\") as output:\n",
    "        for line in my_zip_file.open(contained_file).readlines():\n",
    "            print(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 2\n",
    "import urllib.request\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "\n",
    "url = \"http://websoilsurvey.sc.egov.usda.gov\"\n",
    "headers={'User-Agent':user_agent,} \n",
    "\n",
    "request=urllib.request.Request(url,None,headers) #The assembled request\n",
    "response = urllib.request.urlopen(request)\n",
    "data = response.read() # The data u need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Option 3\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-Agent', 'MyApp/1.0')]\n",
    "urllib.request.install_opener(opener)\n",
    "urllib.request.urlretrieve(\n",
    "  \"http://websoilsurvey.sc.egov.usda.gov/DSD/Download/{CACHENAME}/{SUBFOLDER}\",\n",
    "   \"wss_SSA_AL019_[2021-09-15].zip\")\n",
    "outputFilename = \"C:\\\\Users\\\\trubin\\\\Downloads\" + zipfilename\n",
    "url = urllib.request.urlopen(zipurl)\n",
    "zippedData = url.read()\n",
    "# Save zip file to disk\n",
    "print (\"Saving to \",outputFilename)\n",
    "output = open(outputFilename,'wb')\n",
    "output.write(zippedData)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create txt log of zipfiles downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not download the file if already in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not in the log or the file on website is newer\n",
    "\n",
    "# 1. download\n",
    "# 2. update log\n",
    "# 3. unzip to SSURGO folder\n",
    "# 4. delete the downloaded zipped file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdbe6a345e49dbba1d6ce3231a5791291bc277e174161bd9129a45264752b2ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
